{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOg5uB0QeAW6QBkbpt+4gPA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CassioSperb/Neural-Networks-and-Deep-Learning-Challenege/blob/main/deep_learning_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIKfaom_blJD",
        "outputId": "c961473e-65c2-4cbb-d360-8cf09725ae1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras_tuner in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (3.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (2.32.3)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras_tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras_tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras_tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n",
            "Reloading Tuner from my_dir/intro_to_kt/tuner0.json\n",
            "Best Hyperparameters: {'units': 96, 'num_layers': 3, 'units_0': 96, 'dropout': 0.0, 'learning_rate': 0.001, 'units_1': 96, 'units_2': 64, 'tuner/epochs': 50, 'tuner/initial_epoch': 17, 'tuner/bracket': 3, 'tuner/round': 3, 'tuner/trial_id': '0049'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 22 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215/215 - 1s - 3ms/step - accuracy: 0.7273 - loss: 0.5583\n",
            "Loss: 0.5583006143569946, Accuracy: 0.7272594571113586\n"
          ]
        }
      ],
      "source": [
        "# prompt: how to improve the accuracy of this code? should I do any of the following?\n",
        "# Dropping more or fewer columns.\n",
        "# Creating more bins for rare occurrences in columns.\n",
        "# Increasing or decreasing the number of values for each bin.\n",
        "\n",
        "!pip install keras_tuner\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "from tensorflow import keras\n",
        "from kerastuner.tuners import RandomSearch, Hyperband\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "\n",
        "# Import our dependencies\n",
        "\n",
        "\n",
        "# Import and read the charity_data.csv.\n",
        "application_df = pd.read_csv(\"https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv\")\n",
        "\n",
        "# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n",
        "application_df = application_df.drop(columns=['EIN', 'NAME'])\n",
        "\n",
        "# Determine the number of unique values in each column.\n",
        "unique_value_counts = application_df.nunique()\n",
        "\n",
        "\n",
        "# More robust binning for APPLICATION_TYPE\n",
        "application_type_counts = application_df['APPLICATION_TYPE'].value_counts()\n",
        "cutoff_value_app_type = 100 # Adjust this value\n",
        "application_types_to_replace = list(application_type_counts[application_type_counts < cutoff_value_app_type].index)\n",
        "for app in application_types_to_replace:\n",
        "    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
        "\n",
        "# More robust binning for CLASSIFICATION\n",
        "classification_counts = application_df['CLASSIFICATION'].value_counts()\n",
        "cutoff_value_class = 500 # Adjust this value\n",
        "classifications_to_replace = list(classification_counts[classification_counts < cutoff_value_class].index)\n",
        "\n",
        "for cls in classifications_to_replace:\n",
        "    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,\"Other\")\n",
        "\n",
        "# Convert categorical data to numeric with `pd.get_dummies`\n",
        "application_df = pd.get_dummies(application_df, drop_first=True)\n",
        "\n",
        "# Split our preprocessed data into our features and target arrays\n",
        "y = application_df['IS_SUCCESSFUL'].values\n",
        "X = application_df.drop(columns=['IS_SUCCESSFUL']).values\n",
        "\n",
        "\n",
        "# Split the preprocessed data into a training and testing dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Create a StandardScaler instances\n",
        "scaler = StandardScaler()\n",
        "\n",
        "\n",
        "# Fit the StandardScaler\n",
        "X_scaler = scaler.fit(X_train)\n",
        "\n",
        "# Scale the data\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)\n",
        "\n",
        "def create_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    # Tune the number of units in the first Dense layer\n",
        "    # Choose an optimal value between 32-100\n",
        "    hp_units = hp.Int('units', min_value=32, max_value=100, step=32)\n",
        "    model.add(keras.layers.Dense(units=hp_units, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
        "    # Tune the number of layers\n",
        "    for i in range(hp.Int('num_layers', 1, 3)):  # Add 1 to 3 hidden layers\n",
        "        model.add(keras.layers.Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=100, step=32),\n",
        "                                    activation='relu'))\n",
        "    model.add(keras.layers.Dropout(hp.Float('dropout', 0, 0.5, step=0.1))) # Add dropout\n",
        "    model.add(keras.layers.Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "    # Tune the learning rate for the optimizer\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Use Hyperband tuner for more efficient search\n",
        "tuner = kt.Hyperband(\n",
        "    create_model,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=50,  # Increased max_epochs for more thorough search\n",
        "    factor=3,\n",
        "    directory='my_dir',\n",
        "    project_name='intro_to_kt'\n",
        ")\n",
        "tuner.search(X_train_scaled, y_train, epochs=20, validation_split=0.2)\n",
        "\n",
        "best_hyper = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Best Hyperparameters: {best_hyper.values}\")\n",
        "\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "model_loss, model_accuracy = best_model.evaluate(X_test_scaled, y_test, verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export our model to HDF5 file\n",
        "best_model.save(\"AlphabetSoupCharity.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWUv_tCqb2e6",
        "outputId": "8b0f0a8b-669c-4c9e-e259-60702b027891"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    }
  ]
}